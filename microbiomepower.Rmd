---
title: "Sample size for a microbiome experiment"
author: "George Savva"
date: "27 March 2021"
output: 
    html_document:
      theme: cosmo

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Choosing the number of experimental units (the 'sample size') is an important part of any research study design.  A greater sample size means that estimates from your study will be more precise.  Using too many samples has ethical and resource implications, and so you need to trade off this precision against cost.

A sample size calculation finds the smallest sample size that gives you an acceptable chance of meeting your study aims.

Hence your study aims, and what constitutes an 'acceptable' risk of failure both need to be known.

* Be clear about what the goal of your study is, that is, what question(s) must your study be able to answer.

For confirmatory studies targetting specific features of the microbiome, standard approaches to sample size calculations (power calculations or calculations based on precision of effect estimates) can be adopted.

For exploratory studies, where there are no particular hypothesised effects, sample size calculations are more difficult, since there is no criterion to determine the success or failure of a study.  In this case, you might explore the likely effect sizes detectable at different sample sizes (using previous literature or data as a guide) and then find the likely precision of your own study at different sample sizes.

* Be clear whether your study is confirmatory (aiming to test a specific hypothesis or estimate a specific quantity such as a prevalence) or exploratory (looking for differences)

* For a confirmatory study, that is a study to test specific hypotheses or estimate specific quantities, identify the elements required for a sample size calculation (described below).

* For an exploratory study, use previous data to establish the likely effect sizes for different microbial features detectable by your study at different sample sizes, and find the trade-off between precision and cost that you are satisfied with.  You could either use published effect sizes directly if they are available, or conduct studies by simulation using real or simulated data (see below).

The literature on sample size calculations specific to microbiome studies is extremely limited, but is likely to improve in the near future.  Any justifiable method for calculating sample sizes is acceptable, but important considerations are:

* Being honest about sample size requirements and the feasibility of your study.  Many sample size calculations are fudged to fit budget constraints, by using favourable estimates of standard deviation and effect size.  While this might 

* Make sure your calculation matches your design and your objectives.

* Be wary of *significant* estimates arising from previously published work.  We know that published effect sizes are prone to upward bias, and so taking say 50% of the published effect size as the likely effect size for replication is reasonable.

* Note also that if a previous study returned a p-value of exactly or very close to 0.05, then the study had only 50% power to detect the effect that it reported.  This can be easily seen if you consider that an exact replication of this study would be equally likely to return a p-value above or below the p-value of the original, that is above or below 0.05.  You need to roughly double the size of the previous study to have a well powered replication in this case.

* Consider how generalisable effect sizes and standard errors are likely to be from one technology to another or setting to another.  For example, you might be planning a human study, but have information on likely effect sizes and variance from in vitro colon model studies.  This is likely to be useful, but not directly transferable, so consider what proportion of the effect observed in vitro would be expected, or would still be worth pursing if observed in the follow-up study, and calculate power/sample size according to this.

## How does sample size affect an experiment

Sample size governs the *precision* of the estimates from your study (reflected by, for example, their standard errors).  If you are conducting hypothesis tests, this precision in turn governs the *power* of your study to detect deviations from null hypotheses. 

The power of a study to detect an effect is defined as the probability that the study will successfully identify it, if it in fact exists.  We typically choose to design our studies with 80% or 90% power.  This means that we accept a 10% to 20% risk of missing a true effect.  

The simple equation:

$$s.e. \propto \frac{s.d.}{\sqrt{n}}$$

Describes the relationship between the standard error of an estimate, the standard deviation of the outcome measure used to calculate it, and the sample size used (with the exact relationship depending on study design).  

So, for example, as sample size doubles, standard error of estimates (hence the width of confidence intervals) will decrease by a factor of $\sqrt{2}$.  With respect to power, more precision mean that smaller differences between groups can be detected.  Doubling of sample size will lead to the ability to detect effects that are $\sqrt{2}$ times smaller.

## Information needed for a sample size calculation

To conduct a traditional sample size calculation, you need to know: 

1. **The hypothesis (or hypotheses) that you are planning to test or the parameters you want to estimate.**  These will follow from your study aims.  
2. The study design and analytical methods you will use to address these aims
3. **The likely variation in the outcome measures across samples.**  This will likely come from prior data on a similar outcome in a similar population.  
4. **The precision required, or the size of the effect that you want to be able to reliably detect.**  The precision required or minimum effect sizes can be guided by a combination of prior data (to establish what might be a feasible difference) and your own study aims (to establish what is the smallest difference that would be considered important enough not to want to miss).

And if you will use a 'hypothesis testing' framework in your analysis:

5. **The threshold for statistical significance.**  Our convention is to apply a threshold of p<0.05 for statistical significance.  The most commmon reason for deviating from a critical threshold of 0.05 is an adjustment for multiple testing.  If you are testing many hypotheses (such as many different candidate microbiome features) then you might make a correction to a p-value, controlling instead a false discovery rate (FDR) in which case the required FDR should be known.
6. **The required power.**  How much risk of missing a true effect are you willing to accept, or what proportion of missed true effects are you willing to accept.

# Study design considerations

### Complex designs 1 : Cross-over and paired designs or repeated measure designs with the comparison *within* groups.

If sources of variation can be removed from estimates of the comparions of interest, then studies will be more efficient and power required will be reduced.  The simplest example of this is in paired experiments.

For example, in a paired study, the variance in outcomes between pairs is not important, since we make our comparison within pairs.  

Suppose we have N observations per group, and we are interested in a between groups comparison.  Then if the experimental units vary with standard deviation of $sd_{total}$, then the standard error of the estimate is:

$$se= \sqrt{2}\times\frac{sd}{\sqrt{N}}$$

If we can pair our observations and divide the between unit variance into the variance between pairs and the variance within pairs, and we assign one of each pair to each group, then our estimate has variance:

$$se= \sqrt{2}\times\frac{sd_{within}}{\sqrt{N}}$$

That is, the precision has been improved by a factor of

$$\frac{sd^2}{sd^2_{within}} = \frac{1}{1-ICC}$$
where $ICC$ is intra-class correlation within pairs of observations.

So the effective sample size has been increased by a factor of $\frac{1}{1-ICC}$, so we can reduce the sample size by this same factor.

Note the benefit of using a paired design depends on the ICC.  If there is little intra-class correlation, (eg if pre- and post- measures are largely uncorrelated in an RCT) then nothing is gained for the expense of taking the extra measurement.  If the pairs are very highly correlated, and the ICC is close to 1, then the sample size can be dramatically reduced.

*To be added - the typical ICC for microbiome features in different circumstances*

### Complex designs 2 : Cage-effects and intra-class correlation and repeated measures designs with the comparison *across* groups.

Statistical power (and precision) is determined by the number of independent experimental units. Where units are not treated independently of each other, so called 'intra-class correlations' (ICC) can be introduced, and this affects the calculation of standard error and hence study power and required sample sizes.

ICC between experimental units can be caused by factors that jointly influence specific units (such as a shared environment, shared genetic background, or contamination within a group).  This can caused outcomes within groups to be more similar than we would expect by chance, and so each unit contributes less information to the study than 

For example, we have observed strong intra-class correlations in microbiomes of mice sharing cages, and of fish sharing water systems.  This is unsurprising given the sensitivity of microbiomes to external factors.

The 'design effect' of a study is the factor by which we need to inflate the sample size to take into account intra-class correlation.  The design effect is calculated by:

$\text{Design effect} = 1 + ICC \times (n-1)$

Where ICC is the intra-class correlation, and $n$ is the number of animals per group.  ICC is the proportion of total variation that is attributable to group membership as opposed to individual variation.  It is calculated by:

$ICC = \frac{{sd}^2_{between}}{ {sd}^2_{between} + {sd}^2_{within}}$

For example, an ICC of 0.3 is not uncommon in the microbiomes of mice housed in the same cages (correspond to around 1/3 of variance being cage-specific as opposed to mouse-specific).  So if mice are to be housed five to a cage, then required sample sizes should be inflated by a factor of around $1+.3*(5-1) = 2.2$ to account for this.

### Methods for sample size calculation

For simple research questions based on one or a few microbial features (such as a specific diversity measure or presence or abundance of a particular species), required sample sizes can be calculated using standard formulas.

For example, if your aim is to compare alpha diversity between two independent groups, a power calculation for a simple unpaired t-test would be appropriate. Standard free-to-use tools such as `G*power` or functions from various `R` packges can be used depending on the design.

For more exploratory work, it is difficult to determine a 'required' sample size, since the goals of the study in terms of its success or failure are ambiguous.  The greater the sample size, 

## Power and sample size by simulation

For more complex designs or outcomes, where formulas are not available, sample sizes can be calculated by simulation.  This proceeds as follows:

1. Create 'fake' datasets that are as similar as possible to the real data that you are likely to observe, with typical levels of variation and including the effects that you wish to be able to detect.

2. Analyse these using your planned methods, and find the proportion of datasets in which you were able to successfully detect the effect(s) of interest.

3. Vary the sample size and any other assumptions to check how the power varies.

This method relies on being able to simulate realistically structured datasets with realistic but artificially induced effects.  Fortunately with the increasing amount of microbiome data being publically available this is not unfeasible.

There are two main approaches for this.

# Approaches to power and sample size for typical aims

## Comparisons of alpha diversity

## Comparisons of beta diversity

## Overall comparisons of distributions

## Differential abundance of individual features.

## Account for loss of data



## Precision

## What are the implications of a sample size that is too high, or too low.

## Appendix: A short scoping review of literature on power and sample size in microbiome studies.

### Search terms and results:

A search was conducted using World of Knowledge, and the search terms:  

WoK: 18 results for (TI=microbiome  AND (TI=sample size  OR TI=power))  AND LANGUAGE: (English)
Abstract search reduced this to 11 papers.
12 January 2021

# Summary of each approach:

Chen (2020) POWMIC:
Use real data to estimate the parameters of data to simulate.
Calculate the proportion of true positive and false positive detections.
edgeR and microbiome

Would be good to extend to see the power curve - that is what proportion are detected at what true difference..

Casals-Pascual, C; Gonzalez, A; Vazquez-Baeza, Y; Song, SJ; Jiang, LJ; Knight, R	Casals-Pascual, Climent; Gonzalez, Antonio; Vazquez-Baeza, Yoshiki; Song, Se Jin; Jiang, Lingjing; Knight, Rob	Microbial Diversity in Clinical Microbiome Studies: Sample Size and Statistical Power Considerations	GASTROENTEROLOGY
Casals-Pascual et al 
Phylogenetic diversity and Unifrac:
For beta diversity
Null hypothesis is that the difference in distance between pairs within groups is smaller than the distance between groups.  Power proceeds from comparing the average 'between' bet a diversity to the average 'within' beta diversity.


Kelly, BJ; Gross, R; Bittinger, K; Sherrill-Mix, S; Lewis, JD; Collman, RG; Bushman, FD; Li, HZ	Kelly, Brendan J.; Gross, Robert; Bittinger, Kyle; Sherrill-Mix, Scott; Lewis, James D.; Collman, Ronald G.; Bushman, Frederic D.; Li, Hongzhe	Power and sample-size estimation for microbiome studies using pairwise distances and PERMANOVA	BIOINFORMATICS



La Rosa, PS; Brooks, JP; Deych, E; Boone, EL; Edwards, DJ; Wang, Q; Sodergren, E; Weinstock, G; Shannon, WD	La Rosa, Patricio S.; Brooks, J. Paul; Deych, Elena; Boone, Edward L.; Edwards, David J.; Wang, Qin; Sodergren, Erica; Weinstock, George; Shannon, William D.	Hypothesis Testing and Power Calculations for Taxonomic-Based Human Microbiome Data	PLOS ONE





