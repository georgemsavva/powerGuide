---
title: "Sample size for a microbiome experiment"
author: "George Savva"
date: "13/01/2021"
output: 
    html_document:
      theme: cosmo

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Choosing the number of experimental units (the 'sample size') is an important part of any research study design.  A greater sample size means that estimates from your study will be more precise.  However using too many samples has ethical and resource implications, and so you need to trade off this precision against cost.  

A sample size calculation finds the smallest sample size that gives you an acceptable chance of meeting your study aims.

Hence your study aims, and what constitutes an 'acceptable' risk of failure both need to be clearly known.

## How does sample size affect an experiment

Sample size governs the *precision* of the estimates from your study (reflected by, for example, their standard errors).  If you are conducting hypothesis tests, this precision in turn governs the *power* of your study to detect deviations from null hypotheses. 

The power of a study to detect an effect is defined as the probability that the study will successfully identify it, if it in fact exists.  We typically choose to design our studies with 80% or 90% power.  This means that we accept a 10% to 20% risk of missing a true effect.  

The simple equation:

$$s.e. \propto \frac{s.d.}{\sqrt{n}}$$

Describes the relationship between the standard error of an estimate, the standard deviation of the outcome measure used to calculate it, and the sample size used (with the exact relationship depending on study design).  

So, for example, as sample size doubles, standard error of estimates (hence the width of confidence intervals) will decrease by a factor of $\sqrt{2}$.  With respect to power, more precision mean that smaller differences between groups can be detected.  Doubling of sample size will lead to the ability to detect effects that are $\sqrt{2}$ times smaller.

## Information needed for a sample size calculation

So choosing the sample size requires that you know: 

1. **The hypothesis (or hypotheses) that you are planning to test or the parameters you want to estimate.**  These will follow from your study aims.  
2. The study design and analytical methods you will use to address these aims
3. **The likely variation in the outcome measures.**  This will likely come from prior data on a similar outcome in a similar population.  
4. **The precision required, or the size of the effect that you want to be able to reliably detect.**  The precision required or minimum effect sizes can be guided by a combination of prior data (to establish what might be a feasible difference) and your own study aims (to establish what is the smallest difference that would be considered important enough not to want to miss).

And if you will use a 'hypothesis testing' framework in your analysis:

5. **The threshold for statistical significance.**  Our convention is to apply a threshold of p<0.05 for statistical significance.  The most commmon reason for deviating from a critical threshold of 0.05 is an adjustment for multiple testing.  If you are testing many hypotheses (such as many different candidate microbiome features) then you might make a correction to a p-value, controlling instead a false discovery rate (FDR) in which case the required FDR should be known.
6. **The required power.**  How much risk of missing a true effect are you willing to accept, equivilantly what proportion of missed true effects are you willing to accept.

# Approaches to power and sample size considerations

### Complex designs 1 : Cross-over and paired designs or repeated measure designs with the comparison *within* groups.

If sources of variation can be removed from estimates of the comparions of interest, then studies will be more efficient and power required will be reduced.  The simplest example of this is in paired experiments.

For example, in a paired study, the variance in outcomes between pairs is not important, since we make our comparison within pairs.  

Suppose we have N observations per group, and we are interested in a between groups comparison.  Then if the experimental units vary with standard deviation of $sd_{total}$, then the standard error of the estimate is:

$$se= \sqrt{2}\times\frac{sd}{\sqrt{N}}$$

If we can pair our observations and divide the between unit variance into the variance between pairs and the variance within pairs, and we assign one of each pair to each group, then our estimate has variance:

$$se= \sqrt{2}\times\frac{sd_{within}}{\sqrt{N}}$$

That is, the precision has been improved by a factor of

$$\frac{sd^2}{sd^2_{within}} = \frac{1}{1-ICC}$$
where $ICC$ is intra-class correlation within pairs of observations.

So the effective sample size has been increased by a factor of $\frac{1}{1-ICC}$, so we can reduce the sample size by this same factor.

Note the benefit of using a paired design depends on the 

### Complex designs 2 : Cage-effects and intra-class correlation and repeated measures designs with the comparison *across* groups.

Statistical power (and precision) is detemined by the number of independent experimental units. Where units are not treated independently of each other, so called 'intra-class correlations' (ICC) can be introduced, and this affects the calculation of standard error and hence study power and required sample sizes.

ICC between experimental units can be caused by factors that jointly influence specific units (such as a shared environment, shared genetic background, or contamination within a group).  This can caused outcomes within groups to be more similar than we would expect by chance, and so each unit contributes less information to the study than 

For example, we have observed strong intra-class correlations in microbiomes of mice sharing cages, and of fish sharing water systems.  This is unsurprising given the sensitivity of microbiomes to external factors.

The 'design effect' of a study is the factor by which we need to inflate the sample size to take into account intra-class correlation.  The design effect is calculated by:

$\text{Design effect} = 1 + ICC \times (n-1)$

Where ICC is the intra-class correlation, and $n$ is the number of animals per group.  ICC is the proportion of total variation that is attributable to group membership as opposed to individual variation.  It is calculated by:

$ICC = \frac{{sd}^2_{between}}{ {sd}^2_{between} + {sd}^2_{within}}$

For example, an ICC of 0.3 is not uncommon in the microbiomes of mice housed in the same cages (correspond to around 1/3 of variance being cage-specific as opposed to mouse-specific).  So if mice are to be housed five to a cage, then required sample sizes should be inflated by a factor of around $1+.3*4 = 2.2$ to account for this.

### Methods for sample size calculation

For simple designs and analyses, required sample sizes can be calculated using standard formulas.  For example, if your aim is to compare alpha diversity between two independent groups, a power calculation for a simple unpaired t-test would be appropriate. Standard free to use tools such as 'G*power' or functions from the 'R' 'pwr' and 'pwr2' packages can be used. 

## Example:  Sample size calculation for alpha diversity


## Power and sample size by simulation

For more complex designs or outcomes, where formulas are not available, sample sizes can be calculated by simulation.  This proceeds as follows:

1. Create 'fake' datasets that are as similar as possible to the real data that you are likely to observe, with typical levels of variation and including the effects that you wish to be able to detect.
2. Analyse these using your planned methods, and find the proportion of datasets in which you were able to successfully detect the effect(s) of interest.
3. Vary the sample size and any other assumptions to check how the power varies.

This method relies on being able to simulate realisticly structured datasets with realistic but artificially induced effects.

There are two main approaches for this.  XX presents a guide to achiving this...

# Approaches to power and sample size for typical aims

## Comparisons of alpha diversity

## Comparisons of beta diversity

## Overall comparisons of distributions

## Differential abundance of individual features.

## Account for loss of data



## Precision

## What are the implications of a sample size that is too high, or too low.

## Appendix: A short scoping review of literature on power and sample size in microbiome studies.

### Search terms and results:

A search was conducted using World of Knowledge, and the search terms:  

WoK: 18 results for (TI=microbiome  AND (TI=sample size  OR TI=power))  AND LANGUAGE: (English)
Abstract search reduced this to 11 papers.
12 January 2021

# Summary of each approach:

Chen (2020) POWMIC:
Use real data to estimate the parameters of data to simulate.
Calculate the proportion of true positive and false positive detections.
edgeR and microbiome

Would be good to extend to see the power curve - that is what proportion are detected at what true difference..

Casals-Pascual, C; Gonzalez, A; Vazquez-Baeza, Y; Song, SJ; Jiang, LJ; Knight, R	Casals-Pascual, Climent; Gonzalez, Antonio; Vazquez-Baeza, Yoshiki; Song, Se Jin; Jiang, Lingjing; Knight, Rob	Microbial Diversity in Clinical Microbiome Studies: Sample Size and Statistical Power Considerations	GASTROENTEROLOGY
Casals-Pascual et al 
Phylogenetic diversity and Unifrac:
For beta diversity
Null hypothesis is that the difference in distance between pairs within groups is smaller than the distance between groups.  Power proceeds from comparing the average 'between' bet a diversity to the average 'within' beta diversity.


Kelly, BJ; Gross, R; Bittinger, K; Sherrill-Mix, S; Lewis, JD; Collman, RG; Bushman, FD; Li, HZ	Kelly, Brendan J.; Gross, Robert; Bittinger, Kyle; Sherrill-Mix, Scott; Lewis, James D.; Collman, Ronald G.; Bushman, Frederic D.; Li, Hongzhe	Power and sample-size estimation for microbiome studies using pairwise distances and PERMANOVA	BIOINFORMATICS



La Rosa, PS; Brooks, JP; Deych, E; Boone, EL; Edwards, DJ; Wang, Q; Sodergren, E; Weinstock, G; Shannon, WD	La Rosa, Patricio S.; Brooks, J. Paul; Deych, Elena; Boone, Edward L.; Edwards, David J.; Wang, Qin; Sodergren, Erica; Weinstock, George; Shannon, William D.	Hypothesis Testing and Power Calculations for Taxonomic-Based Human Microbiome Data	PLOS ONE





